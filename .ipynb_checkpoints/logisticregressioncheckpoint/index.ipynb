{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60942e6c516a29541a1535264ae517a7",
     "grade": false,
     "grade_id": "cell-f17628a8eca2cbc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Logistic Regression Checkpoint\n",
    "\n",
    "This checkpoint is designed to test your understanding of the content from the Logistic Regression Cumulative Lab.\n",
    "\n",
    "Specifically, this will cover:\n",
    "\n",
    "* Calculating and interpreting classification metrics, particularly in the context of class imbalance\n",
    "* Performing an end-to-end ML process with logistic regression\n",
    "* Using NumPy and not pandas in a modeling context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d15bc418eb3f8a9ad3482c52a1ad2abe",
     "grade": false,
     "grade_id": "cell-f2bd8816c42b2fb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Your Task: Use Logistic Regression on the Wisconsin Breast Cancer Dataset\n",
    "\n",
    "### Data Understanding\n",
    "\n",
    "Here we will use the Wisconsin Breast Cancer dataset, which is available through scikit-learn ([documentation here](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)).  The goal is to predict whether a breast mass is benign or malignant based on attributes of cell nuclei in a tissue sample. Deeper understanding of the specific attributes is not required for this task.\n",
    "\n",
    "In the cell below, we load this dataset, perform a train-test split, and scale the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbc515aa796d463c33631678814267f8",
     "grade": false,
     "grade_id": "cell-5acfbd990f38a8e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('breast_cancer.csv')\n",
    "# Seperate features from target\n",
    "X, y = df.iloc[:,:-1].to_numpy(), df.iloc[:,-1].to_numpy()\n",
    "# Perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc17cd48c3bea078d27a994ecbf57d9c",
     "grade": false,
     "grade_id": "cell-900fc5185c49d611",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1. Baseline Metrics\n",
    "\n",
    "Before we actually perform any modeling, let's determine what metrics we would expect to get with a \"dummy\" model that always predicts the positive class.\n",
    "\n",
    "For this assessment we'll define \"negative\" as a 0 (benign) and \"positive\" as a 1 (malignant).\n",
    "\n",
    "We will focus on the test data, since this is what we will use to evaluate our actual model as well.\n",
    "\n",
    "The code below shows an array containing the number of records in the test dataset with class 0 (benign) and class 1 (malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf4fcd118e4c575b8889bd33e80dcf71",
     "grade": false,
     "grade_id": "cell-3749265b38571fac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([89, 54], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f631342e2d949cc02de4778a61da6a7b",
     "grade": false,
     "grade_id": "cell-a235418dd6df809c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In other words, a model that always predicts the positive class, will predict a 1 for every observation. Given the imbalance of the target seen above, (The balance is similar in the training data as well), we will calculate different classification metrics to evaluate the model's performance for both positive and negative labels.\n",
    "\n",
    "The confusion matrix looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff5c3cef4fcd679c2ec6ac7a0e7ef3fa",
     "grade": false,
     "grade_id": "cell-f5e5c157e227ff7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=DummyClassifier(strategy='constant', constant=1).fit(X_train, y_train),\n",
    "                                       X=X_test, y=y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56a6937dd02a56814c6ddbd870d2be88",
     "grade": false,
     "grade_id": "cell-6cc083aefe185bc9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For each of the following questions, assume that a \"baseline\" metric means the metric we would find if our model always chose class 1.\n",
    "\n",
    "You can just use the numbers 89 and 54 in your answer; you don't need to use `y_test` directly.\n",
    "\n",
    "#### What is the baseline accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e755b181915a1928635263dbafbea91d",
     "grade": false,
     "grade_id": "cell-4fc8ecbde3cefea5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6223776223776224"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "baseline_accuracy = 89 / (89 + 54)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43b15a2ecf13d6232e02f8c930484716",
     "grade": true,
     "grade_id": "cell-d82e1145566cbde4",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# baseline_accuracy should be a number between 0 and 1\n",
    "assert 0.0 <= baseline_accuracy and baseline_accuracy <= 1.0\n",
    "\n",
    "# PUT ALL WORK FOR THE ABOVE QUESTION ABOVE THIS CELL\n",
    "# THIS UNALTERABLE CELL CONTAINS HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3602d6ea1412f5a93bc2bb96dbc33c8",
     "grade": false,
     "grade_id": "cell-ce29090a1e7a50a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### What is the baseline recall?\n",
    "\n",
    "As a reminder, a \"negative\" prediction is represented as 0 (benign) and a \"positive\" prediction as 1 (malignant). So all baseline predictions will be either \"true positives\" (actually 1, predicted 1) or \"false positives\" (actually 0, predicted 1) and there will not be any \"true negatives\" or \"false negatives\" because this model always chooses 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1afa42887089cbba5fd6ac5ca740e5dc",
     "grade": false,
     "grade_id": "cell-bd2963c096bc719d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "baseline_recall = 1.0\n",
    "# YOUR CODE HERE\n",
    "\n",
    "baseline_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1af441fe5d623b8b83fd999a88aa6296",
     "grade": true,
     "grade_id": "cell-d1e42e6ab2e2dbbe",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# baseline_recall should be a number between 0 and 1\n",
    "assert 0.0 <= baseline_recall and baseline_recall <= 1.0\n",
    "\n",
    "# PUT ALL WORK FOR THE ABOVE QUESTION ABOVE THIS CELL\n",
    "# THIS UNALTERABLE CELL CONTAINS HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fb4fa3032fc6b6031784ba86ad30ad8",
     "grade": false,
     "grade_id": "cell-dba77045ab3d5734",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### What is the baseline precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1565f8e0ad75be5ca0b9f2a4b8ff37b7",
     "grade": false,
     "grade_id": "cell-ad278d44219e8d59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "baseline_precision = 1.0\n",
    "# YOUR CODE HERE\n",
    "\n",
    "baseline_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cad6886e1fa5f31d5470160689d181fe",
     "grade": true,
     "grade_id": "cell-ef0982e7efdef257",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# baseline_precision should be a number between 0 and 1\n",
    "assert 0.0 <= baseline_precision and baseline_precision <= 1.0\n",
    "\n",
    "# PUT ALL WORK FOR THE ABOVE QUESTION ABOVE THIS CELL\n",
    "# THIS UNALTERABLE CELL CONTAINS HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39ed2200b2fe421b52cc606fa37ef63b",
     "grade": false,
     "grade_id": "cell-09a829b393083712",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### What is the baseline f1-score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e4a3f036374389d18a8e63ead446de5",
     "grade": false,
     "grade_id": "cell-b2df160d3ab75209",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "baseline_f1 = 2 * (baseline_precision * baseline_recall) / (baseline_precision + baseline_recall)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "baseline_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79b64c37103336ed2404a9857f99ce10",
     "grade": true,
     "grade_id": "cell-ffaf20b73f713bf7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# baseline_f1 should be a number between 0 and 1\n",
    "assert 0.0 <= baseline_f1 and baseline_f1 <= 1.0\n",
    "\n",
    "# PUT ALL WORK FOR THE ABOVE QUESTION ABOVE THIS CELL\n",
    "# THIS UNALTERABLE CELL CONTAINS HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b61e7f0a41a41b7d2fe29b1fc463af2",
     "grade": false,
     "grade_id": "cell-594415dfc9fb22d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Instantiate and Fit a `LogisticRegression` Model\n",
    "\n",
    "Use the `LogisticRegression` model from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). Specify a `random_state` of 42 but otherwise use default hyperparameters.\n",
    "\n",
    "Because logistic regression applies regularization by default, make sure you use the scaled training data to fit the model.\n",
    "\n",
    "Call this model `model`.\n",
    "\n",
    "We have also included code to display the confusion matrix on the training data; if the confusion matrix doesn't render, that indicates that something is incorrect about your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "179392aebf07a49006f10315bf84fa6c",
     "grade": false,
     "grade_id": "cell-b2952b81667870c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Instantiate the model\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Fit the model on the scaled data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=model, X=X_train_scaled, y=y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e20594b5b08f86873a250a8cacabeed",
     "grade": true,
     "grade_id": "cell-c43d403e0a8efb8c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# model should be a LogisticRegression\n",
    "assert type(model) == LogisticRegression\n",
    "\n",
    "# model should be fitted\n",
    "assert type(model.coef_) == np.ndarray\n",
    "\n",
    "# PUT ALL WORK FOR THE ABOVE QUESTION ABOVE THIS CELL\n",
    "# THIS UNALTERABLE CELL CONTAINS HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use Cross-Validation to Evaluate the Fitted Model\n",
    "\n",
    "Use `cross_val_score` from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)) to evaluate the expected accuracy of the fitted model, prior to using the test data.\n",
    "\n",
    "Use a `cv` of 3 and assign the result to `cv_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ffa59f1e3e77a84d9081358466ee41e",
     "grade": false,
     "grade_id": "cell-3b52c61d710a43d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98591549, 0.97887324, 0.95070423])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=3)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a2f9b6cf1478a800cf07104706ae923",
     "grade": true,
     "grade_id": "cell-10967fd0bba39816",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# cv_scores should contain 3 scores. If it doesn't, double-check\n",
    "# the value passed in for cv\n",
    "assert len(cv_scores) == 3\n",
    "\n",
    "# PUT ALL WORK FOR THE ABOVE QUESTION ABOVE THIS CELL\n",
    "# THIS UNALTERABLE CELL CONTAINS HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffb4d30bba69b8d5fa77c68d2d665b65",
     "grade": false,
     "grade_id": "cell-599a6362c8ef628f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4. Compare Baseline and Fitted Model Scores\n",
    "\n",
    "Now, use functions from scikit-learn to compute the accuracy, recall, precision, and f1-score of the fitted model. We have prepared code that will print them out side-by-side with the baseline scores.\n",
    "\n",
    "Documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score), [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score), [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score), and [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score).\n",
    "\n",
    "This time, **use the test data to calculate each metric.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff64061546476c28a9c26655cf9e2114",
     "grade": false,
     "grade_id": "cell-264adb30a8921fde",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy\n",
      "Baseline: 0.622 Fitted Model: 0.979\n",
      "Recall\n",
      "Baseline: 1.000 Fitted Model: 0.981\n",
      "Precision\n",
      "Baseline: 1.000 Fitted Model: 0.964\n",
      "F1 Score\n",
      "Baseline: 1.000 Fitted Model: 0.972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# Replace None with appropriate code\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "model_accuracy = accuracy_score(y_test, y_pred)\n",
    "model_recall = recall_score(y_test, y_pred)\n",
    "model_precision = precision_score(y_test, y_pred)\n",
    "model_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "Accuracy\n",
    "Baseline: {baseline_accuracy:1.3f} Fitted Model: {model_accuracy:1.3f}\n",
    "Recall\n",
    "Baseline: {baseline_recall:1.3f} Fitted Model: {model_recall:1.3f}\n",
    "Precision\n",
    "Baseline: {baseline_precision:1.3f} Fitted Model: {model_precision:1.3f}\n",
    "F1 Score\n",
    "Baseline: {baseline_f1:1.3f} Fitted Model: {model_f1:1.3f}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09dcd60da46d529519135e8892d32bb7",
     "grade": true,
     "grade_id": "cell-c91e7c7e516f8d41",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# all scores should be values between 0 and 1\n",
    "assert 0.0 <= model_accuracy and model_accuracy <= 1.0\n",
    "assert 0.0 <= model_recall and model_recall <= 1.0\n",
    "assert 0.0 <= model_precision and model_precision <= 1.0\n",
    "assert 0.0 <= model_f1 and model_f1 <= 1.0\n",
    "\n",
    "# PUT ALL WORK FOR THE ABOVE QUESTION ABOVE THIS CELL\n",
    "# THIS UNALTERABLE CELL CONTAINS HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c94107dbb6f31e400bb19858c9768759ef7c8600537d1d879eb7d5b058e9ee3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
